{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pre import Training_data\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exis, import them!\n",
      "data: test_data.npy| labels: test_labels.npy\n"
     ]
    }
   ],
   "source": [
    "_AMOUNT_CLASSE = 11\n",
    "try:\n",
    "    #import model\n",
    "    model1 = keras.models.load_model('model1')\n",
    "    model2 = keras.models.load_model('model2')\n",
    "except:\n",
    "    print('model is missing or wrong name')\n",
    "\n",
    "#making object of custom class\n",
    "test_data = Training_data('test',_AMOUNT_CLASSE)\n",
    "\n",
    "#importing the test data\n",
    "test_dices = np.load('test_data.npy')\n",
    "test_labels = np.load('test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "test_dices, test_labels = test_data.shuffle_2_arrays(test_dices,test_labels)\n",
    "test_labels = test_labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict \n",
    "prediction1 = model1.predict(test_dices)\n",
    "prediction2 = model2.predict(test_dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(prediction):\n",
    "    _predition = []\n",
    "    for i in range(len(prediction)):\n",
    "        _predition.append(np.argmax(prediction[i]))\n",
    "    return _predition\n",
    "\n",
    "\n",
    "_predition1 = get_prediction(prediction1)\n",
    "_predition2 = get_prediction(prediction2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      1.00      1.00        23\n",
      "           2       1.00      1.00      1.00        40\n",
      "           3       1.00      1.00      1.00        36\n",
      "           4       1.00      1.00      1.00        26\n",
      "           5       1.00      1.00      1.00        22\n",
      "           6       1.00      1.00      1.00        26\n",
      "           7       1.00      1.00      1.00        30\n",
      "           8       1.00      1.00      1.00        37\n",
      "           9       1.00      1.00      1.00        31\n",
      "          10       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00       330\n",
      "   macro avg       1.00      1.00      1.00       330\n",
      "weighted avg       1.00      1.00      1.00       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=test_labels,y_pred=_predition1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        29\n",
      "           1       1.00      1.00      1.00        23\n",
      "           2       1.00      1.00      1.00        40\n",
      "           3       1.00      1.00      1.00        36\n",
      "           4       1.00      1.00      1.00        26\n",
      "           5       1.00      1.00      1.00        22\n",
      "           6       1.00      1.00      1.00        26\n",
      "           7       1.00      1.00      1.00        30\n",
      "           8       1.00      1.00      1.00        37\n",
      "           9       1.00      1.00      1.00        31\n",
      "          10       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00       330\n",
      "   macro avg       1.00      1.00      1.00       330\n",
      "weighted avg       1.00      1.00      1.00       330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=test_labels,y_pred=_predition2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 40,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 36,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 26,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 22,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 26,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 30,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 37,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 31,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm1 = confusion_matrix(test_labels,_predition1)\n",
    "cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 40,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 36,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 26,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 22,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 26,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 30,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 37,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 31,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm2 = confusion_matrix(test_labels,_predition2)\n",
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "844750d5e47ed69586078e214d510ee037c49cfe135224478769d7a6fad57b7d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('gpu_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
